# Following the conventions of FunSearch paper, we generate Weibull datasets as follows:
#
# "The Weibull datasets were generated by sampling from a Weibull(45, 3) distribution.
# The parameters of this distribution were chosen based on standard values in the literature [33].
# We further clipped the samples at 100 and rounded all items sizes to the nearest integer in {1, 2, . . . , 100}.
# We generated a training dataset containing 5 instances each with 5_000 items and a validation dataset with the same
# number of instances and items. To test our learned heuristics, we generated test sets of 5 instances with
# 5_000 items, 5 instances of 10_000 items and 1 instance of 100_000 items
# (referred to as Weibull 5k, Weibull 10k and Weibull 100k respectively)."
#
# Additional information: the capacity is of bins is set to 100.

import numpy as np
import pickle


def _generate_weibull_instance(num_items):
    weibull_data = np.random.weibull(3, num_items) * 45
    weibull_data = np.clip(weibull_data, 1, 100)
    weibull_data = np.round(weibull_data).astype(int)
    return weibull_data


def generate_weibull_dataset(num_instances, num_items):
    ret = {}
    for i in range(num_instances):
        key = f'instance_{i}'
        instance = _generate_weibull_instance(num_items)
        instance = {'capacity': 100, 'num_items': num_items, 'items': instance}
        ret[key] = instance
    return ret


def _generate_dataset_and_save(dataset_name: str, num_instances: int, num_items: int, save_file: str):
    dataset = generate_weibull_dataset(num_instances, num_items)
    dataset = {dataset_name: dataset}
    with open(save_file, 'wb') as f:
        pickle.dump(dataset, f)


def _generate_test_dataset():
    np.random.seed(2024)
    _generate_dataset_and_save('weibull_5k', 5, 5000, 'weibull_5k.pkl')
    _generate_dataset_and_save('weibull_10k', 5, 10_000, 'weibull_10k.pkl')
    _generate_dataset_and_save('weibull_100k', 1, 100_000, 'weibull_100k.pkl')


def _l1_bound(items: tuple[int, ...], capacity: int) -> float:
    """Computes L1 lower bound on OPT for bin packing.

    Args:
      items: Tuple of items to pack into bins.
      capacity: Capacity of bins.

    Returns:
      Lower bound on number of bins required to pack items.
    """
    return np.ceil(np.sum(items) / capacity)


def _l1_bound_dataset(instances: dict) -> float:
    """Computes the mean L1 lower bound across a dataset of bin packing instances.

    Args:
      instances: Dictionary containing a set of bin packing instances.

    Returns:
      Average L1 lower bound on number of bins required to pack items.
    """
    l1_bounds = []
    for name in instances:
        instance = instances[name]
        l1_bounds.append(_l1_bound(instance['items'], instance['capacity']))
    return np.mean(l1_bounds)


def cal_bin_paking_bound_dataset():
    np.random.seed(2024)
    inputs0 = generate_weibull_dataset(5, 5_000)
    inputs1 = generate_weibull_dataset(5, 10_000)
    inputs2 = generate_weibull_dataset(1, 100_000)
    all_weibull_data = [inputs0, inputs1, inputs2]
    res = []
    for data in all_weibull_data:
        res.append(_l1_bound_dataset(data))
    print(res)


if __name__ == '__main__':
    # file_train = 'weibull_train.pkl'
    # file_valid = 'weibull_5k_valid.pkl'
    # _generate_dataset_and_save('weibull_5k_train', num_instances=5, num_items=5_000, save_file=file_train)
    # _generate_dataset_and_save('weibull_5k_valid', num_instances=5, num_items=5_000, save_file=file_valid)
    # cal_bin_paking_bound_dataset()
    # _generate_test_dataset()

    with open('weibull_train.pkl', 'rb') as f:
        data = pickle.load(f)['weibull_5k_train']
        print(_l1_bound_dataset(data))